""" 
    
    Visual intersection-over-union tracker
    source https://github.com/bochinski/iou-tracker/blob/master/viou_tracker.py 

    IoUTracker http://elvera.nue.tu-berlin.de/files/1517Bochinski2017.pdf 
    VIoUTracker http://elvera.nue.tu-berlin.de/files/1547Bochinski2018.pdf 
"""

import os
from collections import deque
import cv2 as cv 
import numpy as np
from lapsolver import solve_dense
from tqdm import tqdm
from time import time
from glob import glob 
from multitracker.be import video
from multitracker.tracking import inference
from multitracker.tracking.deep_sort import deep_sort_app
from multitracker.tracking.deep_sort.application_util import visualization
from multitracker.tracking.keypoint_tracking import tracker as keypoint_tracking
from multitracker.keypoint_detection import roi_segm

def iou(bbox1, bbox2):
    """
    Calculates the intersection-over-union of two bounding boxes.
    Args:
        bbox1 (numpy.array, list of floats): bounding box in format x1,y1,x2,y2.
        bbox2 (numpy.array, list of floats): bounding box in format x1,y1,x2,y2.
    Returns:
        int: intersection-over-onion of bbox1, bbox2
    """

    bbox1 = [float(x) for x in bbox1]
    bbox2 = [float(x) for x in bbox2]

    (x0_1, y0_1, x1_1, y1_1) = bbox1
    (x0_2, y0_2, x1_2, y1_2) = bbox2

    # get the overlap rectangle
    overlap_x0 = max(x0_1, x0_2)
    overlap_y0 = max(y0_1, y0_2)
    overlap_x1 = min(x1_1, x1_2)
    overlap_y1 = min(y1_1, y1_2)

    # check if there is an overlap
    if overlap_x1 - overlap_x0 <= 0 or overlap_y1 - overlap_y0 <= 0:
        return 0

    # if yes, calculate the ratio of the overlap to each ROI size and the unified size
    size_1 = (x1_1 - x0_1) * (y1_1 - y0_1)
    size_2 = (x1_2 - x0_2) * (y1_2 - y0_2)
    size_intersection = (overlap_x1 - overlap_x0) * (overlap_y1 - overlap_y0)
    size_union = size_1 + size_2 - size_intersection

    return size_intersection / size_union
    
from multitracker.tracking.upperbound_tracker import Tracker, OpenCVTrack, tlhw2chw
from multitracker.tracking.viou.vis_tracker import VisTracker

class VIoUTracker(Tracker):
    def __init__(self, sigma_l, sigma_h, sigma_iou, t_min, ttl, tracker_type, keep_upper_height_ratio):
        """ V-IOU Tracker.
        See "Extending IOU Based Multi-Object Tracking by Visual Information by E. Bochinski, T. Senst, T. Sikora" for
        more information.
        Args:
            detections (list): list of detections per frame, usually generated by util.load_mot
            sigma_l (float): low detection threshold.
            sigma_h (float): high detection threshold.
            sigma_iou (float): IOU threshold.
            t_min (float): minimum track length in frames.
            ttl (float): maximum number of frames to perform visual tracking.
                        this can fill 'gaps' of up to 2*ttl frames (ttl times forward and backward).
            tracker_type (str): name of the visual tracker to use. see VisTracker for more details.
            keep_upper_height_ratio (float): float between 0.0 and 1.0 that determines the ratio of height of the object
                                            to track to the total height of the object used for visual tracking.
        Returns:
            list: list of tracks.
        """
        self.sigma_l, self.sigma_h, self.sigma_iou, self.t_min, self.ttl, self.tracker_type, self.keep_upper_height_ratio = sigma_l, sigma_h, sigma_iou, t_min, ttl, tracker_type, keep_upper_height_ratio
        if tracker_type == 'NONE':
            assert self.ttl == 1, "ttl should not be larger than 1 if no visual tracker is selected"

        self.tracks_active = []
        self.tracks_extendable = []
        self.tracks_finished = []
        self.frame_buffer = []
        self.last_means = {}
        self.total_count = 0 
        
    def step(self, ob):
        debug = bool(0) 
        frame = ob['img']
        [detected_boxes,scores, features] = ob['detections']
        frame_num = ob['frame_idx']

        self.frame_buffer.append(frame)
        if len(self.frame_buffer) > self.ttl + 1:
            self.frame_buffer.pop(0)

        # apply low threshold to detections
        #dets = [det for det in detections_frame if det['score'] >= sigma_l]
        dets = []
        for i in range(len(scores)):
            if scores[i] > self.sigma_l:
                dets.append({
                    'bbox': tuple(detected_boxes[i]),
                    'score': scores[i],
                    'class': 1 # we only track one class of animals
                })

        track_ids, det_ids = self.associate([t['bboxes'][-1] for t in self.tracks_active], [d['bbox'] for d in dets], self.sigma_iou)
        updated_tracks = []
        for track_id, det_id in zip(track_ids, det_ids):
            self.tracks_active[track_id]['bboxes'].append(dets[det_id]['bbox'])
            self.tracks_active[track_id]['max_score'] = max(self.tracks_active[track_id]['max_score'], dets[det_id]['score'])
            self.tracks_active[track_id]['classes'].append(dets[det_id]['class'])
            self.tracks_active[track_id]['det_counter'] += 1

            if self.tracks_active[track_id]['ttl'] != self.ttl:
                # reset visual tracker if active
                self.tracks_active[track_id]['ttl'] = self.ttl
                self.tracks_active[track_id]['visual_tracker'] = None

            updated_tracks.append(self.tracks_active[track_id])

        tracks_not_updated = [self.tracks_active[idx] for idx in set(range(len(self.tracks_active))).difference(set(track_ids))]

        for track in tracks_not_updated:
            if track['ttl'] > 0:
                if track['ttl'] == self.ttl:
                    # init visual tracker
                    track['visual_tracker'] = VisTracker(self.tracker_type, track['bboxes'][-1], self.frame_buffer[-2], self.keep_upper_height_ratio)
                # viou forward update
                ok, bbox = track['visual_tracker'].update(frame)

                if not ok:
                    # visual update failed, track can still be extended
                    self.tracks_extendable.append(track)
                    continue

                track['ttl'] -= 1
                track['bboxes'].append(bbox)
                updated_tracks.append(track)
            else:
                self.tracks_extendable.append(track)

        # update the list of extendable tracks. tracks that are too old are moved to the finished_tracks. this should
        # not be necessary but may improve the performance for large numbers of tracks (eg. for mot19)
        tracks_extendable_updated = []
        for track in self.tracks_extendable:
            if track['start_frame'] + len(track['bboxes']) + self.ttl - track['ttl'] >= frame_num:
                tracks_extendable_updated.append(track)
            elif track['max_score'] >= self.sigma_h and track['det_counter'] >= self.t_min:
                self.tracks_finished.append(track)
            #elif track['det_counter'] < self.t_min:
            #    self.total_count -= 1
        self.tracks_extendable = tracks_extendable_updated

        new_dets = [dets[idx] for idx in set(range(len(dets))).difference(set(det_ids))]
        dets_for_new = []

        for det in new_dets:
            finished = False
            # go backwards and track visually
            boxes = []
            vis_tracker = VisTracker(self.tracker_type, det['bbox'], frame, self.keep_upper_height_ratio)

            for f in reversed(self.frame_buffer[:-1]):
                ok, bbox = vis_tracker.update(f)
                if not ok:
                    # can not go further back as the visual tracker failed
                    break
                boxes.append(bbox)

                # sorting is not really necessary but helps to avoid different behaviour for different orderings
                # preferring longer tracks for extension seems intuitive, LAP solving might be better
                for track in sorted(self.tracks_extendable, key=lambda x: len(x['bboxes']), reverse=True):

                    offset = track['start_frame'] + len(track['bboxes']) + len(boxes) - frame_num
                    # association not optimal (LAP solving might be better)
                    # association is performed at the same frame, not adjacent ones
                    if 1 <= offset <= self.ttl - track['ttl'] and iou(track['bboxes'][-offset], bbox) >= self.sigma_iou:
                        if offset > 1:
                            # remove existing visually tracked boxes behind the matching frame
                            track['bboxes'] = track['bboxes'][:-offset+1]
                        track['bboxes'] += list(reversed(boxes))[1:]
                        track['bboxes'].append(det['bbox'])
                        track['max_score'] = max(track['max_score'], det['score'])
                        track['last_score'] = det['score']
                        track['classes'].append(det['class'])
                        track['ttl'] = self.ttl
                        track['visual_tracker'] = None

                        self.tracks_extendable.remove(track)
                        if track in self.tracks_finished:
                            del self.tracks_finished[self.tracks_finished.index(track)]
                        updated_tracks.append(track)

                        finished = True
                        break
                if finished:
                    break
            if not finished:
                dets_for_new.append(det)

        # create new tracks
        new_tracks = []
        for det in dets_for_new:
            new_tracks.append({'bboxes': [det['bbox']], 'max_score': det['score'], 'last_score':det['score'], 'start_frame': frame_num, 'ttl': self.ttl,
                    'classes': [det['class']], 'det_counter': 1, 'visual_tracker': None, 'track_id': self.total_count})
            self.total_count += 1 
        self.tracks_active = []
        for track in updated_tracks + new_tracks:
            if track['ttl'] == 0:
                self.tracks_extendable.append(track)
            else:
                self.tracks_active.append(track)


        # update internal variables to be compatible with rest
        self.tracks = []
        for active, tt in zip([1,0,0],[self.tracks_active, self.tracks_extendable]):#, self.tracks_finished]):
            for i, tbox in enumerate(tt):
                steps_without_detection = self.ttl - tbox['ttl'] 
                self.tracks.append(OpenCVTrack(tbox['track_id'],tbox['bboxes'][-1],active,steps_without_detection,tbox['bboxes'],tbox['last_score']))#self.last_means[i],matched_track_scores[i]))
            

    '''# finish all remaining active and extendable tracks
    tracks_finished = tracks_finished + \
                    [track for track in tracks_active + tracks_extendable
                    if track['max_score'] >= sigma_h and track['det_counter'] >= t_min]

    # remove last visually tracked frames and compute the track classes
    for track in tracks_finished:
        if ttl != track['ttl']:
            track['bboxes'] = track['bboxes'][:-(ttl - track['ttl'])]
        track['class'] = max(set(track['classes']), key=track['classes'].count)

        del track['visual_tracker']
    '''
    


def run(config, detection_model, encoder_model, keypoint_model,  min_confidence_boxes, min_confidence_keypoints, tracker = None):
    #config['upper_bound'] = None # ---> force VIOU tracker
    
    nms_max_overlap = 1.
    nms_max_overlap = .25

    if 'video' in config and config['video'] is not None:
        video_reader = cv.VideoCapture( config['video'] )
    else:
        video_reader = None 
    total_frame_number = int(video_reader.get(cv.CAP_PROP_FRAME_COUNT))
    print('[*] total_frame_number',total_frame_number)

    video_file_out = inference.get_video_output_filepath(config)
    if config['file_tracking_results'] is None:
        config['file_tracking_results'] = video_file_out.replace('.%s'%video_file_out.split('.')[-1],'.csv')

    if os.path.isfile(video_file_out): os.remove(video_file_out)
    import skvideo.io
    video_writer = skvideo.io.FFmpegWriter(video_file_out, outputdict={
        '-vcodec': 'libx264',  #use the h.264 codec
        '-crf': '0',           #set the constant rate factor to 0, which is lossless
        '-preset':'veryslow'   #the slower the better compression, in princple, try 
                                #other options see https://trac.ffmpeg.org/wiki/Encode/H.264
    }) 

    print('[*] writing video file %s' % video_file_out)
    
    ## initialize tracker for boxes and keypoints
    sigma_l, sigma_h, sigma_iou = 0, 0.5, 0.5
    t_min, ttl, tracker_type, keep_upper_height_ratio = 2, 1, 'CSRT', 1
    tracker = VIoUTracker(sigma_l, sigma_h, sigma_iou, t_min, ttl, tracker_type, keep_upper_height_ratio)
    keypoint_tracker = keypoint_tracking.KeypointTracker()

    frame_idx = -1
    frame_buffer = deque()
    detection_buffer = deque()
    results = []
    running = True 
    scale = None 
    # ignore first 5 frames
    for _ in range(5):
        _, _ = video_reader.read()
    _, frame = video_reader.read()
    [Hframe,Wframe,_] = frame.shape
    visualizer = visualization.Visualization([Wframe, Hframe], update_ms=5, config=config)
    crop_dim = roi_segm.get_roi_crop_dim(config['data_dir'], config['project_id'], config['test_video_ids'].split(',')[0],Hframe)
    
    # fill up initial frame buffer for batch inference
    for ib in range(config['inference_objectdetection_batchsize']-1):
        ret, frame = video_reader.read()
        frame_buffer.append(frame[:,:,::-1]) # trained on TF RGB, cv2 yields BGR

    for frame_idx in tqdm(range(total_frame_number)):
        config['count'] = frame_idx
        # if buffer not empty use preloaded frames and preloaded detections
        
        # fill up frame buffer as you take something from it to reduce lag 
        if video_reader.isOpened():
            ret, frame = video_reader.read()
            if frame is not None:
                frame_buffer.append(frame[:,:,::-1]) # trained on TF RGB, cv2 yields BGR
            else:
                running = False 
        else:
            running = False 
        
        if running:
            if len(detection_buffer) == 0:
                # fill up frame buffer and then detect boxes for complete frame buffer
                dettensor = np.array(list(frame_buffer)).astype(np.float32)
                batch_detections = inference.detect_batch_bounding_boxes(config, detection_model, dettensor, min_confidence_boxes)
                [detection_buffer.append(batch_detections[ib]) for ib in range(config['inference_objectdetection_batchsize'])]
                keypoint_buffer = inference.inference_batch_keypoints(config, keypoint_model, crop_dim, dettensor, detection_buffer, min_confidence_keypoints)
                
            frame = frame_buffer.popleft()
            detections = detection_buffer.popleft()
                
            boxes = np.array([d.tlwh for d in detections])
            scores = np.array([d.confidence for d in detections])
            features = np.array([d.feature for d in detections])
            #indices = preprocessing.non_max_suppression(
            #    boxes, nms_max_overlap, scores)
            #detections = [detections[i] for i in indices]
            #print('[*] found %i detections' % len(detections))
            # Update tracker
            tracker.step({'img':frame,'detections':[boxes, scores, features], 'frame_idx': frame_idx})

            if keypoint_model is None:
                keypoints, tracked_keypoints = [], []
            else:
                keypoints = keypoint_buffer.popleft()
                # update tracked keypoints with new detections
                tracked_keypoints = keypoint_tracker.update(keypoints)

            # Store results.        
            '''for track in tracker.tracks:
                bbox = track.to_tlwh()
                center0, center1, _, _ = tlhw2chw(bbox)
                result = [frame_idx, track.track_id, center0, center1, bbox[0], bbox[1], bbox[2], bbox[3], track.is_confirmed(), track.time_since_update]
                results.append(result)'''
            
            #print('[%i/%i] - %i detections. %i keypoints' % (config['count'], total_frame_number, len(detections), len(keypoints)),[kp for kp in keypoints])
            out = deep_sort_app.visualize(visualizer, frame, tracker, detections, keypoint_tracker, keypoints, tracked_keypoints, crop_dim, results, sketch_file=config['sketch_file'])
            video_writer.writeFrame(cv.cvtColor(out, cv.COLOR_BGR2RGB))
            #video_writer.writeFrame(cv.cvtColor(out, cv.COLOR_BGR2RGB)) #out[:,:,::-1])
            
            if 1:
                cv.imshow("tracking visualization", cv.resize(out,None,None,fx=0.75,fy=0.75))
                cv.waitKey(5)
        
        if frame_idx % 30 == 0:
            with open( config['file_tracking_results'], 'w') as f:
                for result in results:
                    f.write(','.join([str(r) for r in result])+'\n')

